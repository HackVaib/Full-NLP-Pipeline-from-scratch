{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8da8ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love machine learning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff68d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning (lowercase+remove)\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"[^a-z\\s]\",'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0021244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love nlp', 'machine learning is amazing', 'i love machine learning']\n"
     ]
    }
   ],
   "source": [
    "#cleaned corpus\n",
    "cleaned_corpus=[]\n",
    "for sent in corpus:\n",
    "    cleaned_corpus.append(clean_text(sent))\n",
    "\n",
    "print(cleaned_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e94639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "def tokenization(sentence):\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0c357d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'nlp'], ['machine', 'learning', 'is', 'amazing'], ['i', 'love', 'machine', 'learning']]\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "for sent in cleaned_corpus:\n",
    "    tokens.append(tokenization(sent))\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a4d618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "    filtered=[]\n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            filtered.append(word)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5af61b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'nlp'],\n",
       " ['machine', 'learning', 'amazing'],\n",
       " ['love', 'machine', 'learning']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_no_stop=[]\n",
    "for word_list in tokens:\n",
    "    tokens_no_stop.append(remove_stopwords(word_list))\n",
    "\n",
    "tokens_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "80d0934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ad59889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(word_list):\n",
    "    output=[]\n",
    "    for word in word_list:\n",
    "        output.append(stemmer.stem(word))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f0fad6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['love', 'nlp'], ['machin', 'learn', 'amaz'], ['love', 'machin', 'learn']]\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens=[]\n",
    "for word_list in tokens_no_stop:\n",
    "    stemmed_tokens.append(stem_words(word_list))\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08d04ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'nlp', 'machin', 'learn', 'amaz']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "vocab=[]\n",
    "for word_list in stemmed_tokens:\n",
    "    for word in word_list:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "print(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78bb67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentences,vocab):\n",
    "    bow_dict_list=[]\n",
    "    bow_vectors=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq={}\n",
    "        for word in vocab:\n",
    "            freq[word]=0\n",
    "\n",
    "        for word in sent:\n",
    "            if word in vocab:\n",
    "                freq[word]+=1\n",
    "\n",
    "        bow_dict_list.append(freq)\n",
    "\n",
    "        row_vector=[]\n",
    "        for word in vocab:\n",
    "            row_vector.append(freq[word])\n",
    "\n",
    "        bow_vectors.append(row_vector)\n",
    "\n",
    "    return bow_dict_list,bow_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20f8db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_dict,bow_vec=bow(stemmed_tokens,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e28a9b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'love': 1, 'nlp': 1, 'machin': 0, 'learn': 0, 'amaz': 0},\n",
       " {'love': 0, 'nlp': 0, 'machin': 1, 'learn': 1, 'amaz': 1},\n",
       " {'love': 1, 'nlp': 0, 'machin': 1, 'learn': 1, 'amaz': 0}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "726d31a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 0, 0], [0, 0, 1, 1, 1], [1, 0, 1, 1, 0]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4af25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df(bow_vectors,vocab):\n",
    "    df={}\n",
    "    for word in vocab:\n",
    "        df[word]=0\n",
    "\n",
    "    for row in bow_vectors:\n",
    "        for i in range(len(vocab)):\n",
    "            if row[i]>0:\n",
    "                df[vocab[i]]+=1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8881197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=compute_df(bow_vec,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c63f5c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 2, 'nlp': 1, 'machin': 2, 'learn': 2, 'amaz': 1}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7621005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_idf(df,total_docs):\n",
    "    idf={}\n",
    "    for word,value in df.items():\n",
    "        idf[word]=math.log(total_docs/(value+1))\n",
    "\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d3d0ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0.0,\n",
       " 'nlp': 0.4054651081081644,\n",
       " 'machin': 0.0,\n",
       " 'learn': 0.0,\n",
       " 'amaz': 0.4054651081081644}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf=compute_idf(df,len(bow_vec))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62716757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(bow_vectors,vocab,idf):\n",
    "    tfidf=[]\n",
    "    for row in bow_vectors:\n",
    "        tfidf_row=[]\n",
    "        for i in range(len(vocab)):\n",
    "            word=vocab[i]\n",
    "            tf=row[i]\n",
    "            tfidf_row.append(tf*idf[word])\n",
    "\n",
    "        tfidf.append(tfidf_row)\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00678e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.4054651081081644, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.4054651081081644],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vc=compute_tfidf(bow_vec,vocab,idf)\n",
    "tfidf_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "75444e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "nlp\n",
      "machin\n",
      "learn\n",
      "amaz\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(vocab)):\n",
    "    print(vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c398c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tfidf(tfidf_vectors,vocab):\n",
    "    for i in range(len(tfidf_vectors)):\n",
    "        print(f'\\nSentence {i+1}:')\n",
    "\n",
    "        row=tfidf_vectors[i]\n",
    "\n",
    "        for j in range(len(vocab)):\n",
    "            print(f'{vocab[j]}--> {row[j]}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fbc8c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "love--> 0.0\n",
      "nlp--> 0.4054651081081644\n",
      "machin--> 0.0\n",
      "learn--> 0.0\n",
      "amaz--> 0.0\n",
      "\n",
      "Sentence 2:\n",
      "love--> 0.0\n",
      "nlp--> 0.0\n",
      "machin--> 0.0\n",
      "learn--> 0.0\n",
      "amaz--> 0.4054651081081644\n",
      "\n",
      "Sentence 3:\n",
      "love--> 0.0\n",
      "nlp--> 0.0\n",
      "machin--> 0.0\n",
      "learn--> 0.0\n",
      "amaz--> 0.0\n"
     ]
    }
   ],
   "source": [
    "print_tfidf(tfidf_vc,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67b55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
